{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(data, country_type_label):\n",
    "    \n",
    "    \"\"\"\n",
    "    The clean_dataset function takes a dataset and \n",
    "    a country type label as inputs,filters the dataset\n",
    "    to include only specific economic indicators, cleans\n",
    "    the data by dropping irrelevant or missing columns,\n",
    "    interpolates missing values for years where applicable,\n",
    "    and finally returns a cleaned dataset with an additional \n",
    "    column for the country type.\n",
    "\n",
    "    Parameters:\n",
    "    data: A pandas DataFrame containing raw data with \n",
    "    economic indicators for various countries and years.\n",
    "    \n",
    "    country_type_label: A string that labels the dataset \n",
    "    based on the income level of the country (e.g., \n",
    "    'low_income', 'lower_middle', 'upper_middle', 'high_income').\n",
    "\n",
    "    \"\"\"\n",
    "    #variables we are interested in\n",
    "    indicators = [\n",
    "        'GDP growth (annual %)',\n",
    "        'Government expenditure on education, total (% of GDP)',\n",
    "        'Unemployment, total (% of total labor force) (modeled ILO estimate)'\n",
    "    ]\n",
    "    # filter to only include specific economic indicators we are interested in\n",
    "    cleaned_data = data[data['Series Name'].isin(indicators)]\n",
    "    \n",
    "    cleaned_data = cleaned_data.dropna(subset=['Country Name', \\\n",
    "                                    'Country Code', 'Series Name'])\n",
    "\n",
    "    #has too many missing values to be useful\n",
    "    columns_to_drop = ['1960 [YR1960]', '1961 [YR1961]', '1962 [YR1962]', \\\n",
    "                       '1963 [YR1963]', '1964 [YR1964]']\n",
    "    cleaned_data = cleaned_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    \n",
    "    year_columns = [col for col in cleaned_data.columns \\\n",
    "                    if col.split()[0].isdigit()]\n",
    "    cleaned_data[year_columns] =\\\n",
    "    cleaned_data[year_columns].replace(\"..\", pd.NA).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    #linear interpolation to fill in NA\n",
    "    cleaned_data[year_columns] =\\\n",
    "    cleaned_data[year_columns].interpolate(method='linear', axis=0)\n",
    "    #drop any remaining NA rows\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "    #add in a col to label each country's income level\n",
    "    cleaned_data['country_type'] = country_type_label\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the four data source each with different income levels of countries\n",
    "file_paths = [\n",
    "    '94086144-6ad8-4b75-ac26-1b60a764018a_Data.csv',  \n",
    "    '3fd493b6-dfe0-4afd-b296-1b5892e64ba8_Data.csv',  \n",
    "    '579772b1-602f-4cf6-ac0b-1bb1289918f8_Data.csv',  \n",
    "    '5026bb4a-f8ac-4ee9-860c-f83d47a7aded_Data.csv' \n",
    "]\n",
    "\n",
    "country_types = ['low_income', 'lower_middle', 'upper_middle', 'high_income']\n",
    "\n",
    "cleaned_datasets = []\n",
    "\n",
    "#cleaning each file and merging in to one \n",
    "for file_path, country_type in zip(file_paths, country_types):\n",
    "    data = pd.read_csv(file_path)\n",
    "    cleaned_data = clean_dataset(data, country_type)\n",
    "    cleaned_datasets.append(cleaned_data)\n",
    "\n",
    "merged_data = pd.concat(cleaned_datasets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for for analysis, we want to convert the data into a form that is easy to analyze and rename columns so that they can be used as variables. Optimally, we'd like to have `Country Name`, `Year`, and the economic indicators `GDP %`, `Government Education Expenditure %`, and `Unemployment %` all in one row. To accomplish this, we do the following:\n",
    "\n",
    "1. Rename the year columns so they are easier to reference in the future\n",
    "2. Combine the years into one column instead of having them as columns using a melt transform\n",
    "3. Separate the economic indicators into columns of their own using a pivot transform\n",
    "4. Cast data types into their appropriate types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first rename the year columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop was used to make the dictionary for the year --> column mappings\n",
    "# print(merged_data.shape)\n",
    "# print(\"{\")\n",
    "# for yr in range(1965, 2024):\n",
    "#     print(f\"\\\"{yr} [YR{yr}]\\\": \\\"{yr}\\\",\")\n",
    "# print(\"}\")\n",
    "yr_col_mappings = {\n",
    "    \"1965 [YR1965]\": \"1965\",\n",
    "    \"1966 [YR1966]\": \"1966\",\n",
    "    \"1967 [YR1967]\": \"1967\",\n",
    "    \"1968 [YR1968]\": \"1968\",\n",
    "    \"1969 [YR1969]\": \"1969\",\n",
    "    \"1970 [YR1970]\": \"1970\",\n",
    "    \"1971 [YR1971]\": \"1971\",\n",
    "    \"1972 [YR1972]\": \"1972\",\n",
    "    \"1973 [YR1973]\": \"1973\",\n",
    "    \"1974 [YR1974]\": \"1974\",\n",
    "    \"1975 [YR1975]\": \"1975\",\n",
    "    \"1976 [YR1976]\": \"1976\",\n",
    "    \"1977 [YR1977]\": \"1977\",\n",
    "    \"1978 [YR1978]\": \"1978\",\n",
    "    \"1979 [YR1979]\": \"1979\",\n",
    "    \"1980 [YR1980]\": \"1980\",\n",
    "    \"1981 [YR1981]\": \"1981\",\n",
    "    \"1982 [YR1982]\": \"1982\",\n",
    "    \"1983 [YR1983]\": \"1983\",\n",
    "    \"1984 [YR1984]\": \"1984\",\n",
    "    \"1985 [YR1985]\": \"1985\",\n",
    "    \"1986 [YR1986]\": \"1986\",\n",
    "    \"1987 [YR1987]\": \"1987\",\n",
    "    \"1988 [YR1988]\": \"1988\",\n",
    "    \"1989 [YR1989]\": \"1989\",\n",
    "    \"1990 [YR1990]\": \"1990\",\n",
    "    \"1991 [YR1991]\": \"1991\",\n",
    "    \"1992 [YR1992]\": \"1992\",\n",
    "    \"1993 [YR1993]\": \"1993\",\n",
    "    \"1994 [YR1994]\": \"1994\",\n",
    "    \"1995 [YR1995]\": \"1995\",\n",
    "    \"1996 [YR1996]\": \"1996\",\n",
    "    \"1997 [YR1997]\": \"1997\",\n",
    "    \"1998 [YR1998]\": \"1998\",\n",
    "    \"1999 [YR1999]\": \"1999\",\n",
    "    \"2000 [YR2000]\": \"2000\",\n",
    "    \"2001 [YR2001]\": \"2001\",\n",
    "    \"2002 [YR2002]\": \"2002\",\n",
    "    \"2003 [YR2003]\": \"2003\",\n",
    "    \"2004 [YR2004]\": \"2004\",\n",
    "    \"2005 [YR2005]\": \"2005\",\n",
    "    \"2006 [YR2006]\": \"2006\",\n",
    "    \"2007 [YR2007]\": \"2007\",\n",
    "    \"2008 [YR2008]\": \"2008\",\n",
    "    \"2009 [YR2009]\": \"2009\",\n",
    "    \"2010 [YR2010]\": \"2010\",\n",
    "    \"2011 [YR2011]\": \"2011\",\n",
    "    \"2012 [YR2012]\": \"2012\",\n",
    "    \"2013 [YR2013]\": \"2013\",\n",
    "    \"2014 [YR2014]\": \"2014\",\n",
    "    \"2015 [YR2015]\": \"2015\",\n",
    "    \"2016 [YR2016]\": \"2016\",\n",
    "    \"2017 [YR2017]\": \"2017\",\n",
    "    \"2018 [YR2018]\": \"2018\",\n",
    "    \"2019 [YR2019]\": \"2019\",\n",
    "    \"2020 [YR2020]\": \"2020\",\n",
    "    \"2021 [YR2021]\": \"2021\",\n",
    "    \"2022 [YR2022]\": \"2022\",\n",
    "    \"2023 [YR2023]\": \"2023\"\n",
    "}\n",
    "merged_data = merged_data.rename(columns=yr_col_mappings)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we melt the data to have a year attached to each country/economic indicator combo. Subsequently, we pivot to put each economic indicator into a column of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars = [\n",
    "    'Country Name', 'Country Code', 'Series Name', 'Series Code',\n",
    "    'country_type'\n",
    "]\n",
    "year_cols = [\n",
    "    '1965', '1966', '1967', '1968',\n",
    "       '1969', '1970', '1971', '1972',\n",
    "       '1973', '1974', '1975', '1976',\n",
    "       '1977', '1978', '1979', '1980',\n",
    "       '1981', '1982', '1983', '1984',\n",
    "       '1985', '1986', '1987', '1988',\n",
    "       '1989', '1990', '1991', '1992',\n",
    "       '1993', '1994', '1995', '1996',\n",
    "       '1997', '1998', '1999', '2000',\n",
    "       '2001', '2002', '2003', '2004',\n",
    "       '2005', '2006', '2007', '2008',\n",
    "       '2009', '2010', '2011', '2012',\n",
    "       '2013', '2014', '2015', '2016',\n",
    "       '2017', '2018', '2019', '2020',\n",
    "       '2021', '2022', '2023']\n",
    "# melt to have years as a column\n",
    "analysis_df = merged_data.melt(\n",
    "    id_vars=id_vars,\n",
    "    value_vars=year_cols,\n",
    "    var_name='Year',\n",
    "    value_name='value'\n",
    ") \n",
    "\n",
    "# pivot to get column for each economic indicator\n",
    "index = ['Country Name', 'Country Code', 'country_type', 'Year']\n",
    "analysis_df = analysis_df.pivot(\n",
    "    index=index,\n",
    "    columns='Series Name',\n",
    "    values='value'\n",
    ")\n",
    "\n",
    "# have to reset index because pivoting adds more headers\n",
    "# Inspiraiton from : https://stackoverflow.com/questions/28337117/how-to-pivot-a-dataframe-in-pandas\n",
    "analysis_df = analysis_df.rename_axis(columns=None).reset_index()\n",
    "\n",
    "col_name_mappings = {\n",
    "    'Country Name': 'country_name',\n",
    "    'Country Code': 'country_code',\n",
    "    'country_type': 'income_class',\n",
    "    'Year': 'year',\n",
    "    'GDP growth (annual %)': 'gdp_growth_pct',\n",
    "    'Government expenditure on education, total (% of GDP)': 'govt_exp_edu_pct',\n",
    "    'Unemployment, total (% of total labor force) (modeled ILO estimate)': 'unemp_pct',\n",
    "}\n",
    "analysis_df = analysis_df.rename(columns=col_name_mappings)\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check that all types are appropriate for their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types are not all appropriate, have to convert them\n",
    "analysis_df['year'] = analysis_df['year'].astype('int64')\n",
    "analysis_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct our second hypothesis test. We first drop the GDP growth indicator, as it is not relevant for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "developing_country_catgrs = ['low_income', 'lower_middle']\n",
    "developed_country_catgrs = ['upper_middle', 'high_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the gdp_growth_pct because it is not relevant for this analysis\n",
    "# for dropping one column: https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-in-pandas\n",
    "analysis_df = analysis_df.drop('gdp_growth_pct', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop rows that have NaN in any of their columns to avoid any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN values from analysis\n",
    "# analysis_df.loc[analysis_df['unemp_pct'].isna() | analysis_df['govt_exp_edu_pct'].isna() | analysis_df['gdp_growth_pct'].isna()]\n",
    "analysis_df = analysis_df.dropna().reset_index(drop=True)\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create dummy variables for each category except `high_income`, as this is our reference variable. These dummy variables then get added to our analysis dataframe for the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for each category of income, and add to a new df\n",
    "# high_income is reference var\n",
    "income_dummies = pd.get_dummies(analysis_df['income_class'], drop_first=True)\n",
    "income_dummies = income_dummies.astype('int32')\n",
    "# print(income_dummies)\n",
    "analysis_with_dummies = pd.concat([analysis_df, income_dummies], axis=1)\n",
    "analysis_with_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[['govt_exp_edu_pct', 'unemp_pct']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data=analysis_df, x='govt_exp_edu_pct',\n",
    "            y='unemp_pct', marker=\"o\")\n",
    "plt.xlabel(\"Government expenditure on education, total (% of GDP)\")\n",
    "plt.ylabel(\"Unemployment, total (% of total labor force)\")\n",
    "# plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the experiment as `govt_exp_edu_pct` $\\sigma \\sim$ (`unemp_pct` + `high_income` + `low_income` + `lower_middle` + `upper_middle` + `high_income` * `unemp_pct` + `low_income` * `unemp_pct` + `lower_middle` * `unemp_pct` + `upper_middle` * `unemp_pct`). To add interaction variables, we multiply the columns manually and add them to the analysis dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add multiplied columns to df so use in regression\n",
    "# analysis_with_dummies['high_income_x_unemp_pct'] = analysis_with_dummies['high_income'] * analysis_with_dummies['unemp_pct']\n",
    "analysis_with_dummies['low_income_x_unemp_pct'] = analysis_with_dummies['low_income'] * analysis_with_dummies['unemp_pct']\n",
    "analysis_with_dummies['lower_middle_x_unemp_pct'] = analysis_with_dummies['lower_middle'] * analysis_with_dummies['unemp_pct']\n",
    "analysis_with_dummies['upper_middle_x_unemp_pct'] = analysis_with_dummies['upper_middle'] * analysis_with_dummies['unemp_pct']\n",
    "analysis_with_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Linear Regression. So exciting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [\n",
    "    'unemp_pct', 'low_income', 'lower_middle',\n",
    "    'upper_middle', 'low_income_x_unemp_pct',\n",
    "    'lower_middle_x_unemp_pct', 'upper_middle_x_unemp_pct'\n",
    "]\n",
    "\n",
    "hyp2_model = LinearRegression().fit(\n",
    "    analysis_with_dummies[input_cols],\n",
    "    analysis_with_dummies['govt_exp_edu_pct'])\n",
    "print(hyp2_model.coef_)\n",
    "for i in range(len(input_cols)):\n",
    "    print(f\"The model's {input_cols[i]} coefficient is {round(hyp2_model.coef_[i], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs to see if that makes a difference\n",
    "# Source: HW4 B11\n",
    "def Normalizer(df_cols):\n",
    "    scaler = preprocessing.StandardScaler().fit(df_cols)\n",
    "    return(scaler.transform(df_cols))\n",
    "hyp2_model = LinearRegression().fit(\n",
    "    Normalizer(analysis_with_dummies[input_cols]),\n",
    "    analysis_with_dummies['govt_exp_edu_pct'])\n",
    "print(hyp2_model.coef_)\n",
    "for i in range(len(input_cols)):\n",
    "    print(f\"The model's {input_cols[i]} coefficient is {round(hyp2_model.coef_[i], 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
